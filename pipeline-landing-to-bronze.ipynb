{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32cf4662-8673-4376-bbeb-3f804fbd7ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Pipeline from Landing to Bronze\n",
    "\n",
    "Just read `.csv` files and transform them into delta tables on bronze layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf9d3ae-d604-4f54-ad3d-d1f02f4601da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341d3047-a15f-4ab3-8817-ee5abf90c74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, col, lit\n",
    "from pyspark.sql.connect.dataframe import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from src.common import (\n",
    "    get_tz,\n",
    "    add_reference_date_parameters,\n",
    "    get_reference_dates,\n",
    "    read_sql_template,\n",
    "    table_exists, \n",
    "    use_schema_and_create_if_not_exists,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eddf4b95-0b7b-4d8c-831c-ffde2b14aec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Define Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba942f0-b803-44b9-9ecf-50d101bf30f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"precos_pmc\"\n",
    "SCHEMA = \"bronze\"\n",
    "tz = get_tz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b812b4c4-7b78-45bd-9001-937afb37e69e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "add_reference_date_parameters(dbutils.widgets)\n",
    "use_schema_and_create_if_not_exists(spark, catalog=CATALOG, schema=SCHEMA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f8b94b-4a70-4397-bcc5-cac9af54f820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Collect Dates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c62dcd94-6d75-4bd4-bf09-3b9a347074c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dates = get_reference_dates(dbutils.widgets)\n",
    "print('Running for:', [dt.strftime('%Y-%m-%d') for dt in dates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e92a73f-f281-4d14-b4b3-2def9bbcb017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Full Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2825db68-f1d5-4895-986c-bd41210f3e49",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"filename\":406},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767812153862}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_full_db_dataframe_from_s3_landing(\n",
    "    date_reference: datetime\n",
    ") -> DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Read Full Database file from S3\n",
    "        \n",
    "        It was used before of 2023-07-18\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filepattern = 'Clique_Economia_-_Base_de_Dados.csv'\n",
    "\n",
    "    paths = (\n",
    "        \"s3a://precos-pmc-landing\"\n",
    "        + f\"/{date_reference.strftime('%Y')}\" # YYYY\n",
    "        + f\"/{date_reference.strftime('%m')}\" # MM\n",
    "        + f\"/{date_reference.strftime('%d')}\" # DD\n",
    "        + f\"/{date_reference.strftime('%Y-%m-%d')}_{filepattern}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Lazy\n",
    "        df = (\n",
    "            spark.read\n",
    "\n",
    "                # Read all\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .csv(paths)\n",
    "                \n",
    "                # Extract filename\n",
    "                .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "                .withColumn(\n",
    "                    \"filename\",\n",
    "                    regexp_extract(col(\"_metadata.file_path\"), r\"([^/]+$)\", 1)\n",
    "                )\n",
    "                .drop(\"file_path\")\n",
    "        )\n",
    "\n",
    "        # Materialize\n",
    "        df.count()\n",
    "\n",
    "        # Return materialized\n",
    "        return df\n",
    "\n",
    "    except AnalysisException as e:\n",
    "\n",
    "        if \"PATH_NOT_FOUND\" in e.getMessage():\n",
    "            print(f\"Landing file not found (expected): {paths}\")\n",
    "            return spark.createDataFrame([], StructType([]))\n",
    "        else:\n",
    "            # Anything else is a real failure\n",
    "            raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error reading {filepattern}: {e}')\n",
    "        return spark.createDataFrame([], StructType([]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d036b5-29fd-4703-8b2b-bb19eba28d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Daily database\n",
    "\n",
    "After 2023-07-18, the datasets were daily snapshots, and no longer the full database. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ececae-2457-4715-ab06-8f605e08a33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_incremental_db_dataframe_from_s3_landing(\n",
    "    date_reference: datetime\n",
    ") -> DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Read Incremental Database file from S3\n",
    "        \n",
    "        It is being used after of 2023-07-18\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filepattern = 'Clique_Economia_-_Cotacoes_-_Base_de_Dados.csv'\n",
    "\n",
    "    paths = (\n",
    "        \"s3a://precos-pmc-landing\"\n",
    "        + f\"/{date_reference.strftime('%Y')}\" # YYYY\n",
    "        + f\"/{date_reference.strftime('%m')}\" # MM\n",
    "        + f\"/{date_reference.strftime('%d')}\" # DD\n",
    "        + f\"/{date_reference.strftime('%Y-%m-%d')}_{filepattern}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "\n",
    "                # Read all\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .option(\"delimiter\", \";\")\n",
    "                .option(\"encoding\", \"ISO-8859-1\")\n",
    "                .csv(paths)\n",
    "                \n",
    "                # Extract filename\n",
    "                .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "                .withColumn(\n",
    "                    \"filename\",\n",
    "                    regexp_extract(col(\"_metadata.file_path\"), r\"([^/]+$)\", 1)\n",
    "                )\n",
    "                .drop(\"file_path\")\n",
    "        )\n",
    "\n",
    "        # Materialize\n",
    "        df.count()\n",
    "\n",
    "        # Return materialized\n",
    "        return df\n",
    "\n",
    "    except AnalysisException as e:\n",
    "\n",
    "        if \"PATH_NOT_FOUND\" in e.getMessage():\n",
    "            print(f\"Landing file not found (expected): {paths}\")\n",
    "            return spark.createDataFrame([], StructType([]))\n",
    "        else:\n",
    "            # Anything else is a real failure\n",
    "            raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error reading {filepattern}: {e}')\n",
    "        return spark.createDataFrame([], StructType([]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7fab71-fefe-4676-baee-e94b989c3158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Products database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9c1024-8e5f-45e2-8129-a764d99b7676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_product_db_dataframe_from_s3_landing(\n",
    "    date_reference: datetime\n",
    ") -> DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Read Product Database file from S3\n",
    "        \n",
    "        It is being used before 2023-07-18\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filepattern = 'Clique_Economia_-_Produto_-_Base_de_Dados.csv'\n",
    "\n",
    "    paths = (\n",
    "        \"s3a://precos-pmc-landing\"\n",
    "        + f\"/{date_reference.strftime('%Y')}\" # YYYY\n",
    "        + f\"/{date_reference.strftime('%m')}\" # MM\n",
    "        + f\"/{date_reference.strftime('%d')}\" # DD\n",
    "        + f\"/{date_reference.strftime('%Y-%m-%d')}_{filepattern}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "\n",
    "                # Read all\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .csv(paths)\n",
    "                \n",
    "                # Extract filename\n",
    "                .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "                .withColumn(\n",
    "                    \"filename\",\n",
    "                    regexp_extract(col(\"_metadata.file_path\"), r\"([^/]+$)\", 1)\n",
    "                )\n",
    "                .drop(\"file_path\")\n",
    "        )\n",
    "\n",
    "        # Materialize\n",
    "        df.count()\n",
    "\n",
    "        # Return materialized\n",
    "        return df\n",
    "\n",
    "    except AnalysisException as e:\n",
    "\n",
    "        if \"PATH_NOT_FOUND\" in e.getMessage():\n",
    "            print(f\"Landing file not found (expected): {paths}\")\n",
    "            return spark.createDataFrame([], StructType([]))\n",
    "        else:\n",
    "            # Anything else is a real failure\n",
    "            raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error reading {filepattern}: {e}')\n",
    "        return spark.createDataFrame([], StructType([]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e95237-9d11-4624-b572-4674e1210a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register on bronze catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1afc2e-6674-44db-a522-931692a7aba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def table_exists(catalog: str, schema: str, table: str) -> bool:\n",
    "    return (\n",
    "        spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\")\n",
    "             .filter(f\"tableName = '{table}'\")\n",
    "             .count() > 0\n",
    "    )\n",
    "\n",
    "def write_bronze_delta_table_on_s3(\n",
    "    df: DataFrame, \n",
    "    date_reference: datetime, \n",
    "    table_name: str\n",
    ") -> bool:\n",
    "\n",
    "    if df.isEmpty():\n",
    "        print(f\"Nothing to insert in {table_name}\")\n",
    "        return False\n",
    "\n",
    "    s3_path = (\n",
    "        \"s3a://precos-pmc-bronze\"\n",
    "        f\"/{table_name}\"\n",
    "    )\n",
    "\n",
    "    (y, m, d) = (\n",
    "        date_reference.year,\n",
    "        date_reference.month,\n",
    "        date_reference.day\n",
    "    )\n",
    "\n",
    "    full_table_name = f\"{CATALOG}.{SCHEMA}.{table_name}\"\n",
    "\n",
    "    df_with_partitions = (\n",
    "        df\n",
    "        .withColumn(\"year\", lit(y))\n",
    "        .withColumn(\"month\", lit(m))\n",
    "        .withColumn(\"day\", lit(d))\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Create table\n",
    "        if not table_exists(CATALOG, SCHEMA, table_name):\n",
    "            print(f\"Creating table {full_table_name}\")\n",
    "            (\n",
    "                df_with_partitions\n",
    "                .writeTo(full_table_name)\n",
    "                .using(\"delta\")\n",
    "                .partitionedBy(\"year\", \"month\", \"day\")\n",
    "                .create()\n",
    "            )\n",
    "    \n",
    "        # Grants idempotency replaces\n",
    "        else:\n",
    "\n",
    "            print(f\"Replacing partition {y}-{m}-{d} in {full_table_name}\")\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                DELETE FROM {full_table_name}\n",
    "                WHERE year = {y} AND month = {m} AND day = {d}\n",
    "            \"\"\")\n",
    "                    \n",
    "            \n",
    "            (\n",
    "                df_with_partitions\n",
    "                .writeTo(full_table_name)\n",
    "                .append()\n",
    "            )\n",
    "\n",
    "        print(f'Sucessful inserted into {full_table_name}')\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error while writing bronze delta ({table_name}): {e}')\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0296aa3c-bcfc-44ca-b327-62b8b324fc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Execution flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c5e318-d0fe-45ac-88d8-c459d81b54ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for date in dates:\n",
    "    \n",
    "  df_product = get_product_db_dataframe_from_s3_landing(date_reference=date)\n",
    "  df_product = write_bronze_delta_table_on_s3(df=df_product, date_reference=date, table_name='produtos')\n",
    "\n",
    "  df_incremental_db = get_full_db_dataframe_from_s3_landing(date_reference=date)\n",
    "  df_incremental_db = write_bronze_delta_table_on_s3(df=df_incremental_db, date_reference=date, table_name='base_incremental')\n",
    "\n",
    "  df_day = get_incremental_db_dataframe_from_s3_landing(date_reference=date)\n",
    "  df_day = write_bronze_delta_table_on_s3(df=df_day, date_reference=date, table_name='cotacoes')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8023040681596382,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline-landing-to-bronze",
   "widgets": {
    "date_reference": {
     "currentValue": "",
     "nuid": "304fb1ae-39d3-49af-a79e-d50f5d95dbc9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Date Reference",
      "name": "date_reference",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Date Reference",
      "name": "date_reference",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "reprocess_everything": {
     "currentValue": "False",
     "nuid": "0ff5291f-3544-4f46-8f9a-611f37f2bac0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "Reprocess Everything",
      "name": "reprocess_everything",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "Reprocess Everything",
      "name": "reprocess_everything",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
