{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7842472f-0652-46fa-9f09-213956fa3be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.connect.dataframe import DataFrame\n",
    "\n",
    "from common import (\n",
    "    get_tz,\n",
    "    add_reference_date_parameters,\n",
    "    get_reference_dates,\n",
    "    read_sql_template,\n",
    "    table_exists, \n",
    "    use_schema_and_create_if_not_exists,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ac9833-3a9a-4519-a6d8-d3299a7ef00e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CATALOG = \"precos_pmc\"\n",
    "SCHEMA = \"silver\"\n",
    "tz = get_tz()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f037438-0097-4358-b431-270ad2c77902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "add_reference_date_parameters(dbutils.widgets)\n",
    "use_schema_and_create_if_not_exists(spark, catalog=CATALOG, schema=SCHEMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8adc773-b726-467b-9834-4e09b28da1c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dates = get_reference_dates(dbutils.widgets)\n",
    "print('Running for:', [dt.strftime('%Y-%m-%d') for dt in dates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a10903-42ca-45b1-9dbe-91e477583f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_silver_delta_table_on_s3(\n",
    "    spark,\n",
    "    df: DataFrame, \n",
    "    str_list_of_quoted_date_references: str, \n",
    "    table_name: str\n",
    ") -> bool:\n",
    "\n",
    "    if df.isEmpty():\n",
    "        print(f\"Nothing to insert in {table_name}\")\n",
    "        return False\n",
    "\n",
    "    full_table_name = f\"{CATALOG}.{SCHEMA}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Create table\n",
    "        if not table_exists(spark, CATALOG, SCHEMA, table_name):\n",
    "            print(f\"Creating table {full_table_name}\")\n",
    "            (\n",
    "                df\n",
    "                .writeTo(full_table_name)\n",
    "                .using(\"delta\")\n",
    "                .partitionedBy(\"dt_referencia\")\n",
    "                .create()\n",
    "            )\n",
    "    \n",
    "        # Grants idempotency replaces\n",
    "        else:\n",
    "\n",
    "            print(f\"Replacing partition(s) {str_list_of_quoted_date_references} in {full_table_name}\")\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                DELETE FROM {full_table_name} \n",
    "                WHERE dt_referencia IN ({str_list_of_quoted_date_references})\n",
    "            \"\"\")        \n",
    "            \n",
    "            (\n",
    "                df\n",
    "                .writeTo(full_table_name)\n",
    "                .append()\n",
    "            )\n",
    "\n",
    "        print(f'Sucessful inserted into {full_table_name}')\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error while writing {SCHEMA} delta ({table_name}): {e}')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c906b8e-db4d-4b5d-9db6-e2969eb26f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "str_list_of_quoted_date_references = str([dt.strftime('%Y-%m-%d') for dt in dates]).replace('[', '').replace(']', '')\n",
    "\n",
    "\n",
    "sql_cotacoes = read_sql_template('bronze_to_silver_cotacoes.sql', list_of_quoted_date_references=str_list_of_quoted_date_references)\n",
    "df_cotacoes = spark.sql(sql_cotacoes)\n",
    "\n",
    "sql_base_incremental = read_sql_template('bronze_to_silver_base_incremental.sql', list_of_quoted_date_references=str_list_of_quoted_date_references)\n",
    "df_base_incremental = spark.sql(sql_base_incremental)\n",
    "\n",
    "df = df_cotacoes\n",
    "\n",
    "if not df_base_incremental.isEmpty() and not df.isEmpty():\n",
    "    print('merging')\n",
    "    df = df_cotacoes.unionByName(df_base_incremental)\n",
    "\n",
    "if not df_base_incremental.isEmpty() and df.isEmpty():\n",
    "    print('incremental', df_base_incremental.count())\n",
    "    df = df_base_incremental\n",
    "\n",
    "write_silver_delta_table_on_s3(spark, df=df, str_list_of_quoted_date_references=str_list_of_quoted_date_references, table_name='cotacoes')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5588493157356928,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Criação do pipeline bronze para silver",
   "widgets": {
    "date_reference": {
     "currentValue": "",
     "nuid": "ec06ed70-752c-42be-81b7-0bd4c6ac568e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Date Reference",
      "name": "date_reference",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Date Reference",
      "name": "date_reference",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "reprocess_everything": {
     "currentValue": "True",
     "nuid": "838ed0ee-be1d-4184-b773-1c1b7836310c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "Reprocess Everything",
      "name": "reprocess_everything",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "Reprocess Everything",
      "name": "reprocess_everything",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
