{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32cf4662-8673-4376-bbeb-3f804fbd7ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Pipeline da camada landing para camada bronze\n",
    "\n",
    "Apenas transforma .csvs em delta tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf9d3ae-d604-4f54-ad3d-d1f02f4601da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Libs necessárias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341d3047-a15f-4ab3-8817-ee5abf90c74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, col, last, lit\n",
    "from pyspark.sql.connect.dataframe import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType\n",
    "from delta.tables import DeltaTable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f8b94b-4a70-4397-bcc5-cac9af54f820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Coleta datas do databricks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344db5cf-3f6d-4764-92d5-fcefd92e19b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.widgets.text(\"date_reference\", \"\", \"Date Reference\")\n",
    "dbutils.widgets.dropdown(\"reprocess_everything\", \"False\", [\"True\", \"False\"], \"Reprocess Everything\")\n",
    "\n",
    "date_reference_str = dbutils.widgets.get(\"date_reference\")\n",
    "reprocess_everything = True if dbutils.widgets.get(\"reprocess_everything\") == 'True' else False\n",
    "\n",
    "tz = pytz.timezone(\"America/Sao_Paulo\")\n",
    "\n",
    "yesterday = datetime.now(tz) - timedelta(days=2)\n",
    "first_day = datetime(2022, 6, 21).astimezone(tz)\n",
    "\n",
    "date_reference = datetime.fromisoformat(date_reference_str).astimezone(tz) if date_reference_str else yesterday\n",
    "\n",
    "dates = [date_reference]\n",
    "if reprocess_everything:\n",
    "    for date in range((date_reference - first_day).days):\n",
    "        dates.append(first_day + timedelta(days=date + 1))\n",
    "\n",
    "dates.sort(reverse=False)\n",
    "\n",
    "print('Running for: ', [dt.strftime('%Y-%m-%d') for dt in dates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eddf4b95-0b7b-4d8c-831c-ffde2b14aec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define catalogo do Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1050fe-454a-442e-9d80-8b5f2d122bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CATALOG = \"precos_pmc\"\n",
    "SCHEMA = \"bronze\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA} MANAGED LOCATION 's3://precos-pmc-bronze'\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e92a73f-f281-4d14-b4b3-2def9bbcb017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Base de dados completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2825db68-f1d5-4895-986c-bd41210f3e49",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"filename\":406},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767812153862}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_full_db_dataframe_from_s3_landing(\n",
    "    date_reference: datetime\n",
    ") -> DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Read Full Database file from S3\n",
    "        \n",
    "        It was used before of 2023-07-18\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filepattern = 'Clique_Economia_-_Base_de_Dados.csv'\n",
    "\n",
    "    paths = (\n",
    "        \"s3a://precos-pmc-landing\"\n",
    "        + f\"/{date_reference.strftime('%Y')}\" # YYYY\n",
    "        + f\"/{date_reference.strftime('%m')}\" # MM\n",
    "        + f\"/{date_reference.strftime('%d')}\" # DD\n",
    "        + f\"/{date_reference.strftime('%Y-%m-%d')}_{filepattern}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Lazy\n",
    "        df = (\n",
    "            spark.read\n",
    "\n",
    "                # Read all\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .csv(paths)\n",
    "                \n",
    "                # Extract filename\n",
    "                .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "                .withColumn(\n",
    "                    \"filename\",\n",
    "                    regexp_extract(col(\"_metadata.file_path\"), r\"([^/]+$)\", 1)\n",
    "                )\n",
    "                .drop(\"file_path\")\n",
    "        )\n",
    "\n",
    "        # Materialize\n",
    "        df.count()\n",
    "\n",
    "        # Return materialized\n",
    "        return df\n",
    "\n",
    "    except AnalysisException as e:\n",
    "\n",
    "        if \"PATH_NOT_FOUND\" in e.getMessage():\n",
    "            print(f\"Landing file not found (expected): {paths}\")\n",
    "            return spark.createDataFrame([], StructType([]))\n",
    "        else:\n",
    "            # Anything else is a real failure\n",
    "            raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error reading {filepattern}: {e}')\n",
    "        return spark.createDataFrame([], StructType([]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d036b5-29fd-4703-8b2b-bb19eba28d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Base de dados diária\n",
    "\n",
    "Após 18/07/2023, as bases de dados passaram a apenas a cotação diária, sem carregar toda a base de dados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ececae-2457-4715-ab06-8f605e08a33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_incremental_db_dataframe_from_s3_landing(\n",
    "    date_reference: datetime\n",
    ") -> DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Read Incremental Database file from S3\n",
    "        \n",
    "        It is being used after of 2023-07-18\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filepattern = 'Clique_Economia_-_Cotacoes_-_Base_de_Dados.csv'\n",
    "\n",
    "    paths = (\n",
    "        \"s3a://precos-pmc-landing\"\n",
    "        + f\"/{date_reference.strftime('%Y')}\" # YYYY\n",
    "        + f\"/{date_reference.strftime('%m')}\" # MM\n",
    "        + f\"/{date_reference.strftime('%d')}\" # DD\n",
    "        + f\"/{date_reference.strftime('%Y-%m-%d')}_{filepattern}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "\n",
    "                # Read all\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .option(\"delimiter\", \";\")\n",
    "                .option(\"encoding\", \"ISO-8859-1\")\n",
    "                .csv(paths)\n",
    "                \n",
    "                # Extract filename\n",
    "                .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "                .withColumn(\n",
    "                    \"filename\",\n",
    "                    regexp_extract(col(\"_metadata.file_path\"), r\"([^/]+$)\", 1)\n",
    "                )\n",
    "                .drop(\"file_path\")\n",
    "        )\n",
    "\n",
    "        # Materialize\n",
    "        df.count()\n",
    "\n",
    "        # Return materialized\n",
    "        return df\n",
    "\n",
    "    except AnalysisException as e:\n",
    "\n",
    "        if \"PATH_NOT_FOUND\" in e.getMessage():\n",
    "            print(f\"Landing file not found (expected): {paths}\")\n",
    "            return spark.createDataFrame([], StructType([]))\n",
    "        else:\n",
    "            # Anything else is a real failure\n",
    "            raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error reading {filepattern}: {e}')\n",
    "        return spark.createDataFrame([], StructType([]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7fab71-fefe-4676-baee-e94b989c3158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Base de dados de produtos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9c1024-8e5f-45e2-8129-a764d99b7676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_product_db_dataframe_from_s3_landing(\n",
    "    date_reference: datetime\n",
    ") -> DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Read Product Database file from S3\n",
    "        \n",
    "        It is being used before 2023-07-18\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filepattern = 'Clique_Economia_-_Produto_-_Base_de_Dados.csv'\n",
    "\n",
    "    paths = (\n",
    "        \"s3a://precos-pmc-landing\"\n",
    "        + f\"/{date_reference.strftime('%Y')}\" # YYYY\n",
    "        + f\"/{date_reference.strftime('%m')}\" # MM\n",
    "        + f\"/{date_reference.strftime('%d')}\" # DD\n",
    "        + f\"/{date_reference.strftime('%Y-%m-%d')}_{filepattern}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "\n",
    "                # Read all\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .csv(paths)\n",
    "                \n",
    "                # Extract filename\n",
    "                .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "                .withColumn(\n",
    "                    \"filename\",\n",
    "                    regexp_extract(col(\"_metadata.file_path\"), r\"([^/]+$)\", 1)\n",
    "                )\n",
    "                .drop(\"file_path\")\n",
    "        )\n",
    "\n",
    "        # Materialize\n",
    "        df.count()\n",
    "\n",
    "        # Return materialized\n",
    "        return df\n",
    "\n",
    "    except AnalysisException as e:\n",
    "\n",
    "        if \"PATH_NOT_FOUND\" in e.getMessage():\n",
    "            print(f\"Landing file not found (expected): {paths}\")\n",
    "            return spark.createDataFrame([], StructType([]))\n",
    "        else:\n",
    "            # Anything else is a real failure\n",
    "            raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error reading {filepattern}: {e}')\n",
    "        return spark.createDataFrame([], StructType([]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e95237-9d11-4624-b572-4674e1210a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Registra dados no catalogo (bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1afc2e-6674-44db-a522-931692a7aba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def table_exists(catalog: str, schema: str, table: str) -> bool:\n",
    "    return (\n",
    "        spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\")\n",
    "             .filter(f\"tableName = '{table}'\")\n",
    "             .count() > 0\n",
    "    )\n",
    "\n",
    "def write_bronze_delta_table_on_s3(\n",
    "    df: DataFrame, \n",
    "    date_reference: datetime, \n",
    "    table_name: str\n",
    ") -> bool:\n",
    "\n",
    "    if df.isEmpty():\n",
    "        print(f\"Nothing to insert in {table_name}\")\n",
    "        return False\n",
    "\n",
    "    s3_path = (\n",
    "        \"s3a://precos-pmc-bronze\"\n",
    "        f\"/{table_name}\"\n",
    "    )\n",
    "\n",
    "    (y, m, d) = (\n",
    "        date_reference.year,\n",
    "        date_reference.month,\n",
    "        date_reference.day\n",
    "    )\n",
    "\n",
    "    full_table_name = f\"{CATALOG}.{SCHEMA}.{table_name}\"\n",
    "\n",
    "    df_with_partitions = (\n",
    "        df\n",
    "        .withColumn(\"year\", lit(y))\n",
    "        .withColumn(\"month\", lit(m))\n",
    "        .withColumn(\"day\", lit(d))\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Create table\n",
    "        if not table_exists(CATALOG, SCHEMA, table_name):\n",
    "            print(f\"Creating table {full_table_name}\")\n",
    "            (\n",
    "                df_with_partitions\n",
    "                .writeTo(full_table_name)\n",
    "                .using(\"delta\")\n",
    "                .partitionedBy(\"year\", \"month\", \"day\")\n",
    "                .create()\n",
    "            )\n",
    "    \n",
    "        # Grants idempotency replaces\n",
    "        else:\n",
    "\n",
    "            print(f\"Replacing partition {y}-{m}-{d} in {full_table_name}\")\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                DELETE FROM {full_table_name}\n",
    "                WHERE year = {y} AND month = {m} AND day = {d}\n",
    "            \"\"\")\n",
    "                    \n",
    "            \n",
    "            (\n",
    "                df_with_partitions\n",
    "                .writeTo(full_table_name)\n",
    "                .append()\n",
    "            )\n",
    "\n",
    "        print(f'Sucessful inserted into {full_table_name}')\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error while writing bronze delta ({table_name}): {e}')\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0296aa3c-bcfc-44ca-b327-62b8b324fc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fluxo de execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c5e318-d0fe-45ac-88d8-c459d81b54ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for date in dates:\n",
    "    \n",
    "  df_product = get_product_db_dataframe_from_s3_landing(date_reference=date)\n",
    "  df_product = write_bronze_delta_table_on_s3(df=df_product, date_reference=date, table_name='produtos')\n",
    "\n",
    "  df_incremental_db = get_full_db_dataframe_from_s3_landing(date_reference=date)\n",
    "  df_incremental_db = write_bronze_delta_table_on_s3(df=df_incremental_db, date_reference=date, table_name='base_incremental')\n",
    "\n",
    "  df_day = get_incremental_db_dataframe_from_s3_landing(date_reference=date)\n",
    "  df_day = write_bronze_delta_table_on_s3(df=df_day, date_reference=date, table_name='cotacoes')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8023040681596382,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Criação do pipeline para bronze",
   "widgets": {
    "date_reference": {
     "currentValue": "2026-01-13",
     "nuid": "304fb1ae-39d3-49af-a79e-d50f5d95dbc9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Date Reference",
      "name": "date_reference",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Date Reference",
      "name": "date_reference",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "reprocess_everything": {
     "currentValue": "False",
     "nuid": "0ff5291f-3544-4f46-8f9a-611f37f2bac0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "Reprocess Everything",
      "name": "reprocess_everything",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "Reprocess Everything",
      "name": "reprocess_everything",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
