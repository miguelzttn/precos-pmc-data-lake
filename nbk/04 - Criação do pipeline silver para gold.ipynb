{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0646df84-9e54-4dff-9fa4-7e704cd12455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.connect.dataframe import DataFrame\n",
    "\n",
    "from common import (\n",
    "    read_sql_template,\n",
    "    table_exists, \n",
    "    use_schema_and_create_if_not_exists,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0db25ee-b005-4f8d-bbbc-2315e8c2cb43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CATALOG = \"precos_pmc\"\n",
    "SCHEMA = \"gold\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b42a6f-0242-490b-a0f8-b1dc79231d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "use_schema_and_create_if_not_exists(spark, catalog=CATALOG, schema=SCHEMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b586f6-175d-4cdd-a606-2b731cbfb2fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_gold_snapshot_star_schema_delta_table_on_s3(\n",
    "    spark,\n",
    "    df: DataFrame, \n",
    "    table_name: str\n",
    ") -> bool:\n",
    "\n",
    "    if df.isEmpty():\n",
    "        print(f\"Nothing to insert in {table_name}\")\n",
    "        return False\n",
    "\n",
    "    full_table_name = f\"{CATALOG}.{SCHEMA}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Create table\n",
    "        if not table_exists(spark, CATALOG, SCHEMA, table_name):\n",
    "            print(f\"Creating snapshot {full_table_name}\")\n",
    "            (\n",
    "                df\n",
    "                .writeTo(full_table_name)\n",
    "                .using(\"delta\")\n",
    "                .partitionedBy(\"dt_ultima_atualizacao\")\n",
    "                .create()\n",
    "            )\n",
    "    \n",
    "        # Grants idempotency replaces\n",
    "        else:\n",
    "\n",
    "            print(f\"Replacing snapshot in {full_table_name}\")\n",
    "\n",
    "            spark.sql(f\"DELETE FROM {full_table_name}\")        \n",
    "            \n",
    "            (\n",
    "                df\n",
    "                .writeTo(full_table_name)\n",
    "                .append()\n",
    "            )\n",
    "\n",
    "        print(f'Sucessful inserted into {full_table_name}')\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error while writing {SCHEMA} delta ({table_name}): {e}')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281d84b3-20f0-4429-8cbe-17987b7df3cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fact: precos\n",
    "sql_f_precos = read_sql_template('silver_to_gold_f_precos.sql')\n",
    "df_f_precos = spark.sql(sql_f_precos)\n",
    "write_gold_snapshot_star_schema_delta_table_on_s3(spark, df=df_f_precos, table_name='f_precos')\n",
    "\n",
    "# Dimension: produtos\n",
    "sql_d_produtos = read_sql_template('silver_to_gold_d_produtos.sql')\n",
    "df_d_produtos = spark.sql(sql_d_produtos)\n",
    "write_gold_snapshot_star_schema_delta_table_on_s3(spark, df=df_d_produtos, table_name='d_produtos')\n",
    "\n",
    "# Dimension: empresas\n",
    "sql_d_empresas = read_sql_template('silver_to_gold_d_empresas.sql')\n",
    "df_d_empresas = spark.sql(sql_d_empresas)\n",
    "write_gold_snapshot_star_schema_delta_table_on_s3(spark, df=df_d_empresas, table_name='d_empresas')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7247286555355607,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04 - Criação do pipeline silver para gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
